{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>xgBoost linear</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ParamHelpers\n",
      "Warning message:\n",
      "“replacing previous import ‘BBmisc::isFALSE’ by ‘backports::isFALSE’ when loading ‘mlr’”"
     ]
    }
   ],
   "source": [
    "library(mlr)\n",
    "library(parallel)\n",
    "source('../utils.r')\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "data_folder_name  = '../../raw_data'\n",
    "data_file_name    = 'airfoil_self_noise.dat'\n",
    "model_folder_name = '../model'\n",
    "model_file_name   = 'model_regr_xgBoost.RData'\n",
    "ml_algorithm      = \"regr.xgboost\"\n",
    "gs_max_iteration  = 100\n",
    "gs_max_time       = 60*60*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = read_data(data_folder_name %+/% data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRow: 1503\n",
      "NCol: 6"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 800      </td><td>0         </td><td>0.3048    </td><td>71.3      </td><td>0.00266337</td><td>126.201   </td></tr>\n",
       "\t<tr><td>1000      </td><td>0         </td><td>0.3048    </td><td>71.3      </td><td>0.00266337</td><td>125.201   </td></tr>\n",
       "\t<tr><td>1250      </td><td>0         </td><td>0.3048    </td><td>71.3      </td><td>0.00266337</td><td>125.951   </td></tr>\n",
       "\t<tr><td>1600      </td><td>0         </td><td>0.3048    </td><td>71.3      </td><td>0.00266337</td><td>127.591   </td></tr>\n",
       "\t<tr><td>2000      </td><td>0         </td><td>0.3048    </td><td>71.3      </td><td>0.00266337</td><td>127.461   </td></tr>\n",
       "\t<tr><td>2500      </td><td>0         </td><td>0.3048    </td><td>71.3      </td><td>0.00266337</td><td>125.571   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " V1 & V2 & V3 & V4 & V5 & V6\\\\\n",
       "\\hline\n",
       "\t  800       & 0          & 0.3048     & 71.3       & 0.00266337 & 126.201   \\\\\n",
       "\t 1000       & 0          & 0.3048     & 71.3       & 0.00266337 & 125.201   \\\\\n",
       "\t 1250       & 0          & 0.3048     & 71.3       & 0.00266337 & 125.951   \\\\\n",
       "\t 1600       & 0          & 0.3048     & 71.3       & 0.00266337 & 127.591   \\\\\n",
       "\t 2000       & 0          & 0.3048     & 71.3       & 0.00266337 & 127.461   \\\\\n",
       "\t 2500       & 0          & 0.3048     & 71.3       & 0.00266337 & 125.571   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "V1 | V2 | V3 | V4 | V5 | V6 | \n",
       "|---|---|---|---|---|---|\n",
       "|  800       | 0          | 0.3048     | 71.3       | 0.00266337 | 126.201    | \n",
       "| 1000       | 0          | 0.3048     | 71.3       | 0.00266337 | 125.201    | \n",
       "| 1250       | 0          | 0.3048     | 71.3       | 0.00266337 | 125.951    | \n",
       "| 1600       | 0          | 0.3048     | 71.3       | 0.00266337 | 127.591    | \n",
       "| 2000       | 0          | 0.3048     | 71.3       | 0.00266337 | 127.461    | \n",
       "| 2500       | 0          | 0.3048     | 71.3       | 0.00266337 | 125.571    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  V1   V2 V3     V4   V5         V6     \n",
       "1  800 0  0.3048 71.3 0.00266337 126.201\n",
       "2 1000 0  0.3048 71.3 0.00266337 125.201\n",
       "3 1250 0  0.3048 71.3 0.00266337 125.951\n",
       "4 1600 0  0.3048 71.3 0.00266337 127.591\n",
       "5 2000 0  0.3048 71.3 0.00266337 127.461\n",
       "6 2500 0  0.3048 71.3 0.00266337 125.571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat(sprintf('NRow: %d\\nNCol: %d',nrow(data), ncol(data)))\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assunming that target is the last column of the data. \n",
    "If it's not true, one most declare the name of the column that represents the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = names(data)[ncol(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRow: 1503\n",
      "NCol: 6"
     ]
    }
   ],
   "source": [
    "drops = c()\n",
    "data  = data[,c(!(names(data) %in% drops)), with=FALSE]\n",
    "cat(sprintf('NRow: %d\\nNCol: %d',nrow(data), ncol(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Make Task</h2><br>\n",
    "\n",
    "The task encapsulates the data and specifies - through its subclasses - the type of the task. It also contains a description object detailing further aspects of the data. Useful operators are: getTaskFormula, getTaskFeatureNames, getTaskData, getTaskTargets, and subsetTask.\n",
    "\n",
    "Function\n",
    "```R\n",
    "makeRegrTask(id = deparse(substitute(data)), data, target, weights = NULL, blocking = NULL, \n",
    "             fixup.data = \"warn\", check.data = TRUE)\n",
    "```\n",
    "Param.:\n",
    "\n",
    "* data: [data.frame] A data frame containing the features and target variable(s).\n",
    "* target: [character(1)] Name of the target variable.\n",
    "* weights: [numeric] Optional, non-negative case weight vector to be used during fitting. Cannot be set for cost-sensitive learning. Default is NULL which means no (= equal) weights.\n",
    "* fixup.data: [character(1)] Should some basic cleaning up of data be performed? Currently this means removing empty factor levels for the columns. Possible coices are: “no” = Don't do it. “warn” = Do it but warn about it. “quiet” = Do it but keep silent. Default is “warn”.\n",
    "\n",
    "Doc.: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/makeRegrTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in makeTask(type = type, data = data, weights = weights, blocking = blocking, :\n",
      "“Provided data is not a pure data.frame but from class data.table, hence it will be converted.”"
     ]
    }
   ],
   "source": [
    "task = makeRegrTask(data=data, target = target, fixup.data = 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Make Learner</h2><br>\n",
    "Function\n",
    "```R\n",
    "makeLearner(cl, id = cl, predict.type = \"response\", predict.threshold = NULL, \n",
    "            fix.factors.prediction = FALSE, ..., par.vals = list(), config = list())\n",
    "```\n",
    "Param.:\n",
    "\n",
    "* cl: [character(1)] Class of learner. By convention, all regression learners with “regr.”. A list of all integrated learners is available on the learners help page < https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/ >.\n",
    "* predict: [character(1)] “response” (= mean response) or “se” (= standard errors and mean response). Default is “response”.\n",
    "* par.vals: [list] Optional list of named (hyper)parameters. The arguments in ... take precedence over values in this list. We strongly encourage you to use one or the other to pass (hyper)parameters to the learner but not both.\n",
    "\n",
    "Doc.: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/makeLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learner = makeLearner(cl = ml_algorithm, par.vals=list(booster='gblinear', silent=1, nthread=1, eval_metric='mae'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Specifying the search space</h2><br>\n",
    "\n",
    "In order to define a search space, we create a ParamSet object, which describes the parameter space we wish to search. This is done via the function makeParamSet. For each parameter type a special constructor function is available, see: https://www.rdocumentation.org/packages/ParamHelpers/versions/1.10/topics/Param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all parameters:<br>\n",
    "See xgBoost documentation for mor details: http://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          Type len        Def               Constr Req Tunable\n",
       "booster               discrete   -     gbtree gbtree,gblinear,dart   -    TRUE\n",
       "silent                 integer   -          0          -Inf to Inf   -   FALSE\n",
       "eta                    numeric   -        0.3               0 to 1   -    TRUE\n",
       "gamma                  numeric   -          0             0 to Inf   -    TRUE\n",
       "max_depth              integer   -          6             1 to Inf   -    TRUE\n",
       "min_child_weight       numeric   -          1             0 to Inf   -    TRUE\n",
       "subsample              numeric   -          1               0 to 1   -    TRUE\n",
       "colsample_bytree       numeric   -          1               0 to 1   -    TRUE\n",
       "colsample_bylevel      numeric   -          1               0 to 1   -    TRUE\n",
       "num_parallel_tree      integer   -          1             1 to Inf   -    TRUE\n",
       "lambda                 numeric   -          0             0 to Inf   -    TRUE\n",
       "lambda_bias            numeric   -          0             0 to Inf   -    TRUE\n",
       "alpha                  numeric   -          0             0 to Inf   -    TRUE\n",
       "objective              untyped   - reg:linear                    -   -   FALSE\n",
       "eval_metric            untyped   -       rmse                    -   -   FALSE\n",
       "base_score             numeric   -        0.5          -Inf to Inf   -   FALSE\n",
       "missing                numeric   -     <NULL>          -Inf to Inf   -   FALSE\n",
       "nthread                integer   -          -             1 to Inf   -   FALSE\n",
       "nrounds                integer   -          1             1 to Inf   -    TRUE\n",
       "feval                  untyped   -     <NULL>                    -   -   FALSE\n",
       "verbose                integer   -          1               0 to 2   -   FALSE\n",
       "print_every_n          integer   -          1             1 to Inf   Y   FALSE\n",
       "early_stopping_rounds  integer   -     <NULL>             1 to Inf   -   FALSE\n",
       "maximize               logical   -     <NULL>                    -   -   FALSE\n",
       "normalize_type        discrete   -       tree          tree,forest   Y    TRUE\n",
       "rate_drop              numeric   -          0               0 to 1   Y    TRUE\n",
       "skip_drop              numeric   -          0               0 to 1   Y    TRUE\n",
       "                      Trafo\n",
       "booster                   -\n",
       "silent                    -\n",
       "eta                       -\n",
       "gamma                     -\n",
       "max_depth                 -\n",
       "min_child_weight          -\n",
       "subsample                 -\n",
       "colsample_bytree          -\n",
       "colsample_bylevel         -\n",
       "num_parallel_tree         -\n",
       "lambda                    -\n",
       "lambda_bias               -\n",
       "alpha                     -\n",
       "objective                 -\n",
       "eval_metric               -\n",
       "base_score                -\n",
       "missing                   -\n",
       "nthread                   -\n",
       "nrounds                   -\n",
       "feval                     -\n",
       "verbose                   -\n",
       "print_every_n             -\n",
       "early_stopping_rounds     -\n",
       "maximize                  -\n",
       "normalize_type            -\n",
       "rate_drop                 -\n",
       "skip_drop                 -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getParamSet(ml_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_set = makeParamSet(\n",
    "    makeNumericParam(\"eta\",         lower = 0.001, upper = 1),\n",
    "    makeNumericParam(\"gamma\",       lower = 0.001, upper = 1),\n",
    "    makeIntegerParam('max_depth',   lower = 2,   upper = 30),\n",
    "    makeNumericParam(\"subsample\",   lower = 0.1, upper = 1),\n",
    "    makeNumericParam(\"lambda\",      lower = 0, upper = 1),\n",
    "    makeNumericParam(\"lambda_bias\", lower = 0, upper = 1),\n",
    "    makeNumericParam(\"alpha\",       lower = 0, upper = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Specifying the optimization algorithm</h2><br>\n",
    "\n",
    "Once we have specified the search space, we need to choose an optimization algorithm for our parameters. Optimization algorithms are considered TuneControl objects in mlr. The following tuners are available: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/TuneControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimization_algorithm = makeTuneControlGenSA(max.call=gs_max_iteration, max.time=gs_max_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Resampling strategy</h2><br>\n",
    "\n",
    "Function:\n",
    "```R\n",
    "makeResampleDesc(method, predict = \"test\", ..., stratify = FALSE, stratify.cols = NULL)\n",
    "```\n",
    "Param.:\n",
    "\n",
    "* method: [character(1)] “CV” for cross-validation, “LOO” for leave-one-out, “RepCV” for repeated cross-validation, “Bootstrap” for out-of-bag bootstrap, “Subsample” for subsampling, “Holdout” for holdout.\n",
    "* predict: What to predict during resampling: “train”, “test” or “both” sets. Default is “test”.\n",
    "* ... : [any] Further parameters for strategies.\n",
    "    * iters [integer(1)] Number of iterations, for “CV”, “Subsample” and “Boostrap”.\n",
    "    * split [numeric(1)] Proportion of training cases for “Holdout” and “Subsample” between 0 and 1. Default is 2/3.\n",
    "    * reps [integer(1)] Repeats for “RepCV”. Here iters = folds * reps. Default is 10.\n",
    "    * folds [integer(1)] Folds in the repeated CV for RepCV. Here iters = folds * reps. Default is 10.\n",
    "\n",
    "Doc.: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/makeResampleDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resample = makeResampleDesc(method = \"CV\", iters = 3, predict = 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Measures</h2><br>\n",
    "\n",
    "List of performance measures:\n",
    "\n",
    "Doc.: http://mlr-org.github.io/mlr-tutorial/release/html/measures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measures = list(mae, rmse, expvar, timetrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Performing the tuning</h2><br>\n",
    "\n",
    "Optimizes the hyperparameters of a learner. Allows for different optimization methods, such as grid search, evolutionary strategies, iterated F-race, etc. You can select such an algorithm (and its settings) by passing a corresponding control object. For a complete list of implemented algorithms look at TuneControl. Multi-criteria tuning can be done with tuneParamsMultiCrit.\n",
    "\n",
    "Function:\n",
    "```R\n",
    "tuneParams(learner, task, resampling, measures, par.set, control, show.info = getMlrOption(\"show.info\"))\n",
    "```\n",
    "Param.:\n",
    "\n",
    "* learner: [Learner | character(1)] The learner. If you pass a string the learner will be created via makeLearner\n",
    "* task: [Task] The task.\n",
    "* resampling: [ResampleInstance | ResampleDesc] Resampling strategy to evaluate points in hyperparameter space.\n",
    "* measures: [list of Measure | Measure] Performance measures to evaluate. The first measure, aggregated by the first aggregation function is optimized, others are simply evaluated. \n",
    "* par.set: [ParamSet] Collection of parameters and their constraints for optimization. Dependent parameters with a requires field must use quote and not expression to define it.\n",
    "* control: [TuneControl] Control object for search method. Also selects the optimization algorithm for tuning.\n",
    "* show.info: [logical(1)] Print verbose output on console? Default is set via configureMlr.\n",
    "\n",
    "Doc.: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/tuneParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cpus = detectCores(all.tests = FALSE, logical = TRUE) - 1\n",
    "parallelMap::parallelStartMulticore(cpus, show.info=FALSE)\n",
    "\n",
    "tune_result = tuneParams(learner, task, resample, measures, param_set, optimization_algorithm, show.info=FALSE)\n",
    "\n",
    "parallelMap::parallelStop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>mae.test.mean</dt>\n",
       "\t\t<dd>5.06291367330443</dd>\n",
       "\t<dt>rmse.test.rmse</dt>\n",
       "\t\t<dd>6.14123401605647</dd>\n",
       "\t<dt>expvar.test.mean</dt>\n",
       "\t\t<dd>0.0877453224824479</dd>\n",
       "\t<dt>timetrain.test.mean</dt>\n",
       "\t\t<dd>0.0333333333333338</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[mae.test.mean] 5.06291367330443\n",
       "\\item[rmse.test.rmse] 6.14123401605647\n",
       "\\item[expvar.test.mean] 0.0877453224824479\n",
       "\\item[timetrain.test.mean] 0.0333333333333338\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "mae.test.mean\n",
       ":   5.06291367330443rmse.test.rmse\n",
       ":   6.14123401605647expvar.test.mean\n",
       ":   0.0877453224824479timetrain.test.mean\n",
       ":   0.0333333333333338\n",
       "\n"
      ],
      "text/plain": [
       "      mae.test.mean      rmse.test.rmse    expvar.test.mean timetrain.test.mean \n",
       "         5.06291367          6.14123402          0.08774532          0.03333333 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune_result$y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>eta</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>gamma</dt>\n",
       "\t\t<dd>0.794100299477577</dd>\n",
       "\t<dt>max_depth</dt>\n",
       "\t\t<dd>24</dd>\n",
       "\t<dt>subsample</dt>\n",
       "\t\t<dd>0.778018943965435</dd>\n",
       "\t<dt>lambda</dt>\n",
       "\t\t<dd>0.0195771191945377</dd>\n",
       "\t<dt>lambda_bias</dt>\n",
       "\t\t<dd>0.747833609580994</dd>\n",
       "\t<dt>alpha</dt>\n",
       "\t\t<dd>0.839636478403149</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[eta] 1\n",
       "\\item[gamma] 0.794100299477577\n",
       "\\item[max\\textbackslash{}\\_depth] 24\n",
       "\\item[subsample] 0.778018943965435\n",
       "\\item[lambda] 0.0195771191945377\n",
       "\\item[lambda\\textbackslash{}\\_bias] 0.747833609580994\n",
       "\\item[alpha] 0.839636478403149\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "eta\n",
       ":   1gamma\n",
       ":   0.794100299477577max_depth\n",
       ":   24subsample\n",
       ":   0.778018943965435lambda\n",
       ":   0.0195771191945377lambda_bias\n",
       ":   0.747833609580994alpha\n",
       ":   0.839636478403149\n",
       "\n"
      ],
      "text/plain": [
       "        eta       gamma   max_depth   subsample      lambda lambda_bias \n",
       " 1.00000000  0.79410030 24.00000000  0.77801894  0.01957712  0.74783361 \n",
       "      alpha \n",
       " 0.83963648 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unlist(tune_result$x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train A Learning Algorithm with best Hyperparameters</h2><br>\n",
    "\n",
    "Given a Task, creates a model for the learning machine which can be used for predictions on new data.\n",
    "\n",
    "Function:\n",
    "```R\n",
    "train(learner, task, subset, weights = NULL)\n",
    "```\n",
    "Param.:\n",
    "\n",
    "* learner: [Learner | character(1)] The learner. If you pass a string the learner will be created via makeLearner.\n",
    "* task: [Task] The task.\n",
    "* subset: [integer | logical] Selected cases. Either a logical or an index vector. By default all observations are used.\n",
    "* weights: [numeric] Optional, non-negative case weight vector to be used during fitting. If given, must be of same length as subset and in corresponding order. By default NULL which means no weights are used unless specified in the task (Task). Weights from the task will be overwritten.\n",
    "\n",
    "Doc.: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_learner = setHyperPars(tune_result$learner, par.vals = tune_result$x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with all data to make final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train(optimal_learner, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculates Feature Importance Values For Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for learner.id=regr.xgboost; learner.class=regr.xgboost\n",
      "Trained on: task.id = data; obs = 1503; features = 5\n",
      "Hyperparameters: nrounds=1,verbose=0,booster=gblinear,silent=1,nthread=1,eval_metric=mae,eta=1,gamma=0.794,max_depth=24,subsample=0.778,lambda=0.0196,lambda_bias=0.748,alpha=0.84\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in names(object) <- nm: tentativa de especificar um atributo em um NULL\n",
     "output_type": "error",
     "traceback": [
      "Error in names(object) <- nm: tentativa de especificar um atributo em um NULL\nTraceback:\n",
      "1. getFeatureImportance(model)",
      "2. getFeatureImportanceLearner(lrn, object, ...)",
      "3. getFeatureImportanceLearner.regr.xgboost(lrn, object, ...)",
      "4. getFeatureImportanceLearner.classif.xgboost(.learner, .model, \n .     ...)",
      "5. setNames(fiv, imp$Feature)"
     ]
    }
   ],
   "source": [
    "getFeatureImportance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object.size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save final model</h2><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save(model, file = model_folder_name %+/% model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
